{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> # Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the dataset\nwine_df = pd.read_csv('../input/white-wine-quality/winequality-white.csv', sep=';')\nwine_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The description of each features (excluding the output):\n\n*  ***volatile acidity***: Volatile acidity is the gaseous acids present in wine.\n*  ***fixed acidity***: Primary fixed acids found in wine are tartaric, succinic, citric, and malic.\n*  ***residual sugar***: Amount of sugar left after fermentation.\n*  ***citric acid***: It is weak organic acid, found in citrus fruits naturally.\n*  ***chlorides***: Amount of salt present in wine.\n*  ***free sulfur dioxide***: So2 is used for prevention of wine by oxidation and microbial spoilage.\n*  ***total sulfur dioxide***\n*  ***pH***: In wine pH is used for checking acidity\n*  ***density***\n*  ***sulphates***: Added sulfites preserve freshness and protect wine from oxidation, and bacteria.\n*  ***alchohol***: Percent of alcohol present in wine. ","metadata":{}},{"cell_type":"code","source":"wine_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wine_df.describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wine_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# separate the features and target variable\nfeat_df = wine_df.drop(columns=['quality'])\nprint(feat_df.head())\n\ntarget = wine_df['quality']\nprint(target.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot distribution of each features\nfeat_df.hist(bins=20, figsize=(10,20))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.kdeplot(x='residual sugar', fill=True, log_scale=True ,data=feat_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the boxplot for each features\nplt.figure(figsize=(12,8))\nsns.boxplot(data=feat_df, orient='h')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Outliers almost occurs in each features, let's explore further ","metadata":{}},{"cell_type":"code","source":"sns.boxplot(x='pH', data=feat_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x='density', data=feat_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x='chlorides', data=feat_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"small_df = feat_df[['volatile acidity', 'citric acid', 'sulphates']]\n\n# Plot small range feature \nsns.boxplot(data=small_df, orient='v')\nplt.xticks(rotation=45)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each boxplot shows outliers on each features. Let's see if the outliers are affectiong the correlation","metadata":{}},{"cell_type":"code","source":"# Plot heatmap for entire dataset\nplt.figure(figsize=(10,10))\nsns.heatmap(wine_df.corr(), annot=True, fmt='.2f')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above heatmaps shows residual sugar and density are positively highly correlated **(0.84)** and alchohol and density are negatively highly corelated **(-0.78)**.\n\nOther features worth mentioning that correlated (either positive or negative) are:\n* total sulfur dioxide and free sulfur dioxide **(0.62)**\n* density and total sulfur dioxide **(0.53)**\n* quality and alcohol **(0.44)**\n* fixed axidity and pH **(-0.43)**\n* residual sugar and alcohol **(-0.45)**\n* total sulfur dioxide and alcohol **(-0.45)**\n\nThe result of the heatmap may occur because outliers from every features. Thus, almost every features are weak correlated.","metadata":{}},{"cell_type":"code","source":"# Pair plot \nsns.pairplot(data=wine_df, hue='quality', palette='tab10', corner=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the target \nsns.histplot(data=target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wine_df['quality'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, cross_val_score, RepeatedStratifiedKFold\nfrom sklearn.preprocessing import RobustScaler, LabelEncoder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalise the features\nscaler = RobustScaler()\nfeat_df_norm = scaler.fit_transform(feat_df)\nprint(feat_df_norm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Change the label of corresponding target variable\nencoder = LabelEncoder()\ntarget_enc = encoder.fit_transform(target)\nprint({index: label for index,label in enumerate(encoder.classes_)})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = feat_df_norm\ny = target_enc\nprint(X.shape, y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33, stratify=y, random_state=12)\n\nprint('Shape of X_train, y_train: {xtrain}, {ytrain}'.format(xtrain=X_train.shape, ytrain=y_train.shape))\nprint('Shape of X_test, y_test: {xtest}, {ytest}'.format(xtest=X_test.shape, ytest=y_test.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reggresion Model","metadata":{}},{"cell_type":"markdown","source":"We will train and use cross-validation to get better score overall","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.svm import SVR\nimport tensorflow as tf\n\nfrom sklearn.model_selection import RandomizedSearchCV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The list of regression model and corresponding score that will be used:\n* Linear Regression **(74.535%)**\n* KNN **(62.987%)**\n* SVM **(69.554%)**\n* Decision Tree\n* Random Forest\n* Ridge Reggresion\n* Lasso Reggresion\n* Gaussian Reggresion\n* Polynomial Reggresion\n* Neural Network","metadata":{}},{"cell_type":"code","source":"def evaluate_model(X_train, y_train, model, splits=3):\n    cv = RepeatedStratifiedKFold(n_splits=splits, n_repeats=5, random_state=10)\n    scores = cross_val_score(model, X_train, y_train, scoring='neg_mean_squared_error', cv=cv, n_jobs=2)\n    return scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_model(model, X_train, y_train, X_test, y_test):\n    model = model.fit(X_train, y_train)\n    yhat = model.predict(X_test)\n    \n    # Score result\n    rmse_res= mean_squared_error(y_test, yhat, squared=False)\n    \n    print('RMSE Score: ', round(rmse_res*100, 3))\n    return None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Linear Regression","metadata":{}},{"cell_type":"code","source":"linreg = LinearRegression()\nlr_score = evaluate_model(X_train, y_train, model=linreg)\nprint('Score on train test: ', lr_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Result on test set\ntest_model(linreg, X_train, y_train, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## KNN","metadata":{}},{"cell_type":"code","source":"param_grid = {'n_neighbors': range(1,51),\n              'weights': ['uniform', 'distance']}\n\nknn_rand = RandomizedSearchCV(estimator= KNeighborsRegressor(),\n                             param_distributions=param_grid,\n                             scoring='neg_mean_squared_error',\n                             n_jobs=-1, random_state=20)\nknn_rand = knn_rand.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(knn_rand.best_estimator_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I'll use the weights parameters, for n_neighbors I will try to run it manually","metadata":{}},{"cell_type":"code","source":"#Try all n_neigbors parameter from 1-50\nknn_res = []\nfor n in range(1,51):\n    knn = KNeighborsRegressor(n_neighbors=n, weights='distance')\n    \n    knn_scores = evaluate_model(X_train, y_train, model=knn)\n    knn_res.append(np.mean(knn_scores)*100)\n    print('>%d %.3f (%.3f)' % (n, np.mean(knn_scores)*100, np.std(knn_scores)))\nprint(\"Skor validasi tertinggi: \", round(max(knn_res),3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot knn validation\nk_range = [i for i in range(len(knn_res))]\nplt.plot(k_range, knn_res, color='green');\nplt.title('k-NN Learning Curves')\nplt.xlabel('Value of K for KNN')\nplt.ylabel('Cross-Validated Accuracy')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the best KNN\nknn = KNeighborsRegressor(n_neighbors=13, weights='distance')\nknn_score = evaluate_model(X_train, y_train, model=knn)\nprint('Score on train test: ', knn_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_model(knn, X_train, y_train, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SVM Regressor","metadata":{}},{"cell_type":"code","source":"param_grid = {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n              'gamma': ['scale', 'auto']}\n\nsvr_rand = RandomizedSearchCV(estimator= SVR(),\n                             param_distributions=param_grid,\n                             scoring='neg_mean_squared_error',\n                             n_jobs=-1, random_state=20)\nsvr_rand = svr_rand.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(svr_rand.best_estimator_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the best SVR\nsvr = SVR()\nsvr_score = evaluate_model(X_train, y_train, model=svr)\nprint('Score on train test: ', svr_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_model(svr, X_train, y_train, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Decision Tree Regressor","metadata":{}},{"cell_type":"code","source":"param_grid = {'criterion': [\"squared_error\", \"friedman_mse\", \"poisson\"],\n              'max_depth': range(1,11)}\n\nsvr_rand = RandomizedSearchCV(estimator= DecisionTreeRegressor(random_state=12),\n                             param_distributions=param_grid,\n                             scoring='neg_mean_squared_error',\n                             n_jobs=-1, random_state=20)\nsvr_rand = svr_rand.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}